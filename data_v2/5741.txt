thumb a flow chart showing decisions made by a recommendation engine circa 2001 algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes such as privileging one arbitrary group of users over others bias can emerge due to many factors including but not limited to design of algorithm or unintended or unanticipated use or decisions relating to way data is coded collected selected or used to train algorithm algorithmic bias is found across platforms including but not limited to search engine results and social media platforms and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race gender sexuality and ethnicity study of algorithmic bias is most concerned with algorithms that reflect systematic and unfair discrimination this bias has only recently been addressed in legal frameworks such as 2018 european union s general data protection regulation as algorithms expand their ability to organize society politics institutions and behavior sociologists have become concerned with ways in which unanticipated output and manipulation of data can impact physical world because algorithms are often considered to be neutral and unbiased they can inaccurately project greater authority than human expertise and in some cases reliance on algorithms can displace human responsibility for their outcomes bias can enter into algorithmic systems as a result of pre existing cultural social or institutional expectations because of technical limitations of their design or by being used in unanticipated contexts or by audiences who are not considered in software s initial design algorithmic bias has been cited in cases ranging from election outcomes to spread of online hate speech problems in understanding researching and discovering algorithmic bias stem from proprietary nature of algorithms which are typically treated as trade secrets even when full transparency is provided complexity of certain algorithms poses a barrier to understanding their functioning furthermore algorithms may change or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis in many cases even within a single website or application there is no single algorithm to examine but a network of many interrelated programs and data inputs even between users of same service definitions thumb a 1969 diagram for how a simple computer program makes decisions illustrating a very simple algorithm algorithms are difficult to define but may be generally understood as lists of instructions that determine how programs read collect process and analyze data to generate output for a rigorous technical introduction see algorithms advances in computer hardware have led to an increased ability to process store and transmit data this has in turn boosted design and adoption of technologies such as machine learning and artificial intelligence by analyzing and processing data algorithms are backbone of search engines social media websites recommendation engines online retail online advertising and more contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact and question underlying assumptions of an algorithm s neutrality term algorithmic bias describes systematic and repeatable errors that create unfair outcomes such as privileging one arbitrary group of users over others for example a credit score algorithm may deny a loan without being unfair if it is consistently weighing relevant financial criteria if algorithm recommends loans to one group of users but denies loans to another set of nearly identical users based on unrelated criteria and if this behavior can be repeated across multiple occurrences an algorithm can be described as biased this bias may be intentional or unintentional methods bias can be introduced to an algorithm in several ways during assemblage of a dataset data may be collected digitized adapted and entered into a database according to human designed cataloging criteria next programmers assign priorities or hierarchies for how a program assesses and sorts that data this requires human decisions about how data is categorized and which data is included or discarded some algorithms collect their own data based on human selected criteria which can also reflect bias of human designers other algorithms may reinforce stereotypes and preferences as they process and display relevant data for human users for example by selecting information based on previous choices of a similar user or group of users beyond assembling and processing data bias can emerge as a result of design for example algorithms that determine allocation of resources or scrutiny such as determining school placements may inadvertently discriminate against a category when determining risk based on similar users as in credit scores meanwhile recommendation engines that work by associating users with similar users or that make use of inferred marketing traits might rely on inaccurate associations that reflect broad ethnic gender socio economic or racial stereotypes another example comes from determining criteria for what is included and excluded from results this criteria could present unanticipated outcomes for search results such as in flight recommendation software that omits flights that do not follow sponsoring airline s flight paths algorithms may also display an uncertainty bias offering more confident assessments when larger data sets are available this can skew algorithmic processes toward results that more closely correspond with larger samples which may disregard data from underrepresented populations history early critiques thumb this card was used to load software into an old mainframe computer each byte letter a for example is entered by punching holes though contemporary computers are more complex they reflect this human decision making process in collecting and processing data earliest computer programs were designed to mimic human reasoning and deductions and were deemed to be functioning when they successfully and consistently reproduced that human logic in his 1976 book computer power and human reason artificial intelligence pioneer joseph weizenbaum suggested that bias could arise both from data used in a program but also from way a program is coded weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow by following those rules consistently such programs embody law that is enforce a specific way to solve problems rules a computer follows are based on assumptions of a computer programmer for how these problems might be solved that means code could incorporate programmer s imagination of how world works including his or her biases and expectations while a computer program can incorporate bias in this way weizenbaum also noted that any data fed to a machine additionally reflects human decisionmaking processes as data is being selected finally he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret results weizenbaum warned against trusting decisions made by computer programs that a user doesn t understand comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss crucially tourist has no basis of understanding how or why he arrived at his destination and a successful arrival does not mean process is accurate or reliable an early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to st george s hospital medical school per year from 1982 to 1986 based on implementation of a new computer guidance assessment system that denied entry to women and men with foreign sounding names based on historical trends in admissions contemporary critiques and responses though well designed algorithms frequently determine outcomes that are equally or more equitable than decisions of human beings cases of bias still occur and are difficult to predict and analyze complexity of analyzing algorithmic bias has grown alongside complexity of programs and their design decisions made by one designer or team of designers may be obscured among many pieces of code created for a single program over time these decisions and their collective impact on program s output may be forgotten in theory these biases may create new patterns of behavior or scripts in relationship to specific technologies as code interacts with other elements of society biases may also impact how society shapes itself around data points that algorithms require for example if data shows a high number of arrests in a particular area an algorithm may assign more police patrols to that area which could lead to more arrests decisions of algorithmic programs can be seen as more authoritative than decisions of human beings they are meant to assist a process described by author clay shirky as algorithmic authority shirky uses term to describe decision to regard as authoritative an unmanaged process of extracting value from diverse untrustworthy sources such as search results this neutrality can also be misrepresented by language used by experts and media when results are presented to public for example a list of news items selected and presented as trending or popular may be created based on significantly wider criteria than just their popularity because of their convenience and authority algorithms are theorized as a means of delegating responsibility away from humans this can have effect of reducing alternative options compromises or flexibility sociologist scott lash has critiqued algorithms as a new form of generative power in that they are a virtual means of generating actual ends where previously human behavior generated data to be collected and studied powerful algorithms increasingly could shape and define human behaviors concerns over impact of algorithms on society have led to creation of working groups in organizations such as google and microsoft which have co created a working group named fairness accountability and transparency in machine learning ideas from google have included community groups that patrol outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences in recent years study of fairness accountability and transparency fat of algorithms has emerged as its own interdisciplinary research area with an annual conference called fat types pre existing pre existing bias in an algorithm is a consequence of underlying social and institutional ideologies such ideas may influence or create personal biases within individual designers or programmers such prejudices can be explicit and conscious or implicit and unconscious poorly selected input data will influence outcomes created by machines encoding pre existing bias into software can preserve social and institutional bias and without correction could be replicated in all future uses of that algorithm an example of this form of bias is british nationality act program designed to automate evaluation of new uk citizens after 1981 british nationality act program accurately reflected tenets of law which stated that a man is father of only his legitimate children whereas a woman is mother of all her children legitimate or not in its attempt to transfer a particular logic into an algorithmic process inscribed logic of british nationality act into its algorithm which would perpetuate it even if act was eventually repealed technical thumb facial recognition software used in conjunction with surveillance cameras was found to display bias in recognizing asian and black faces over white faces technical bias emerges through limitations of a program computational power its design or other constraint on system such bias can also be a restraint of design for example a search engine that shows three results per screen can be understood to privilege top three results slightly more than next three as in an airline price display another case is software that relies on randomness for fair distributions of results if random number generation mechanism is not truly random it can introduce bias for example by skewing selections toward items at end or beginning of a list a algorithm uses unrelated information to sort results for example a flight pricing algorithm that sorts results by alphabetical order would be biased in favor of american airlines over united airlines opposite may also apply in which results are evaluated in contexts different from which they are collected data may be collected without crucial external context for example when facial recognition software is used by surveillance cameras but evaluated by remote staff in another country or region or evaluated by non human algorithms with no awareness of what takes place beyond camera s field of vision this could create an incomplete understanding of a crime scene for example potentially mistaking bystanders for those who commit crime lastly technical bias can be created by attempting to formalize decisions into concrete steps on assumption that human behavior works in same way for example software weighs data points to determine whether a defendant should accept a plea bargain while ignoring impact of emotion on a jury another unintended result of this form of bias was found in plagiarism detection software turnitin which compares student written texts to information found online and returns a probability score that student s work is copied because software compares long strings of text it is more likely to identify non native speakers of english than native speakers as latter group might be better able to change individual words break up strings of plagiarized text or obscure copied passages through synonyms because it is easier for native speakers to evade detection as a result of technical constraints of software this creates a scenario where turnitin identifies foreign speakers of english for plagiarism while allowing more native speakers to evade detection emergent emergent bias is result of use and reliance on algorithms across new or unanticipated contexts algorithms may not have been adjusted to consider new forms of knowledge such as new drugs or medical breakthroughs new laws business models or shifting cultural norms this may exclude groups through technology without providing clear outlines to understand who is responsible for their exclusion similarly problems may emerge when training data samples fed to a machine by which it models certain conclusions do not align with contexts that an algorithm encounters in real world in 1990 an example of emergent bias was identified in software used to place us medical students into residencies national residency match program algorithm was designed at a time when few married couples would seek residencies together as more women entered medical schools more students were likely to request a residency alongside their partners process called for each applicant to provide a list of preferences for placement across us which was then sorted and assigned when a hospital and an applicant both agreed to a match in case of married couples where both sought residencies algorithm weighed location choices of higher rated partner first result was a frequent assignment of highly preferred schools to first partner and lower preferred schools to second partner rather than sorting for compromises in placement preference additional emergent biases include correlations unpredictable correlations can emerge when large data sets are compared to each other for example data collected about web browsing patterns may align with signals marking sensitive data such as race or sexual orientation by selecting according to certain behavior or browsing patterns end effect would be almost identical to discrimination through use of direct race or sexual orientation data in other cases algorithm draws conclusions from correlations without being able to understand those correlations for example one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia program algorithm did this because it simply compared survival rates asthmatics with pneumonia are at highest risk historically for this same reason hospitals typically give such asthmatics best and most immediate care unanticipated uses emergent bias can occur when an algorithm is used by unanticipated audiences for example machines may require that users can read write or understand numbers or relate to an interface using metaphors that they do not understand these exclusions can become compounded as biased or exclusionary technology is more deeply integrated into society apart from exclusion unanticipated uses may emerge from end user relying on software rather than their own knowledge in one example an unanticipated user group led to algorithmic bias in uk when british national act program was created as a proof of concept by computer scientists and immigration lawyers to evaluate suitability for british citizenship designers had access to legal expertise beyond end users in immigration offices whose understanding of both software and immigration law would likely have been unsophisticated agents administering questions relied entirely on software which excluded alternative pathways to citizenship and used software even after new case laws and legal interpretations led algorithm to become outdated as a result of designing an algorithm for users assumed to be legally savvy on immigration law software s algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by algorithm rather than by more broader criteria of uk immigration law feedback loops emergent bias may also create a feedback loop or recursion if data collected for an algorithm results in real world responses which are fed back into algorithm for example simulations of predictive policing software deployed in oakland california suggested an increased police presence in black neighborhoods based on crime data reported by public simulation showed that public reported crime based on sight of police cars regardless of what police were doing simulation interpreted police car sightings in modeling its predictions of crime and would in turn assign an even larger increase of police presence within those neighborhoods human rights data analysis group which conducted simulation warned that in places where racial discrimination is a factor in arrests such feedback loops could reinforce and perpetuate racial discrimination in policing systems such as those used to recommend online videos or news articles can create feedback loops when users click on content that is suggested by algorithms it influences next set of suggestions over time this may lead to users entering a filter bubble and being unaware of important or useful content impact commercial influences corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies without knowledge of a user who may mistake algorithm as being impartial for example american airlines created a flight finding algorithm in 1980s software presented a range of flights from various airlines to customers but weighed factors that boosted its own flights regardless of price or convenience in testimony to united states congress president of airline stated outright that system was created with intention of gaining competitive advantage through preferential treatment in a 1998 paper describing google it was shown that founders of company adopted a policy of transparency in search results regarding paid placement arguing that advertising funded search engines will be inherently biased towards advertisers and away from needs of consumers this bias would be an invisible manipulation of user voting behavior a series of studies about undecided voters in us and in india found that search engine results were able to shift voting outcomes by about 20 researchers concluded that candidates have no means of competing if an algorithm with or without intent boosted page listings for a rival candidate facebook users who saw messages related to voting were more likely to vote a 2010 randomized trial of facebook users showed a 20 increase 340 000 votes among users who saw messages encouraging voting as well as images of their friends who had voted legal scholar jonathan has warned that this could create a digital gerrymandering effect in elections selective presentation of information by an intermediary to meet its agenda rather than to serve its users if intentionally manipulated gender discrimination in 2016 professional networking site linkedin was discovered to recommend male variations of women s names in response to search queries site did not make similar recommendations in searches for male names for example andrea would bring up a prompt asking if users meant andrew but queries for andrew did not ask if users meant to find andrea company said this was result of an analysis of users interactions with site in 2012 department store franchise target was cited for gathering data points to infer when women customers were pregnant even if they had not announced it and then sharing that information with marketing partners because data had been predicted rather than directly observed or reported company had no legal obligation to protect privacy of those customers web search algorithms have also been accused of bias google s results may prioritize pornographic content in search terms related to sexuality for example lesbian this bias extends to search engine showing popular but sexualized content in neutral searches for example top 25 sexiest women athletes articles displayed as first page results in searches for women athletes in 2017 google adjusted these results along with others that surfaced hate groups racist views child abuse and pornography and other upsetting and offensive content other examples include display of higher paying jobs to male applicants on job search websites researchers have also identified that machine translation exhibits a strong tendency towards male defaults in particular this is observed in fields linked to unbalanced gender distribution including stem p and l c lamb assessing gender bias in machine translation a case study with google translate neural 2019 in fact current machine translation systems fail to reproduce real world distribution of female workers in 2018 amazon com turned off a system it developed to screen job applications when they realized it was biased against women racial and ethnic discrimination algorithms have been criticized as a method for obscuring racial prejudices in decision making because of how certain races and ethnic groups were treated in past data can often contain hidden biases for example black people are likely to receive longer sentences than white people who committed same crime this could potentially mean that a system amplifies original biases in data one example is use of risk assessments in criminal sentencing in united states and parole hearings judges were presented with an algorithmically generated score intended to reflect risk that a prisoner will repeat a crime for time period starting in 1920 and ending in 1970 nationality of a criminals s father was a consideration in those risk assessment scores today these scores are shared with judges in arizona colorado delaware kentucky louisiana oklahoma virginia washington and wisconsin an independent investigation by propublica found that scores were inaccurate 80 of time and disproportionately skewed to suggest blacks to be at risk of relapse 77 more often than whites in 2015 google apologized when black users complained that an image identification algorithm in its photos application identified them as gorillas in 2010 nikon cameras were criticized when image recognition algorithms consistently asked asian users if they were blinking such examples are product of bias in biometric data sets biometric data is drawn from aspects of body including racial features either observed or inferred which can then be transferred into data points speech recognition technology can have different depending on user s accent this may be caused by a lack of training data for speakers of that accent biometric data about race may also be inferred rather than observed for example a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records regardless of whether there is any police record of that individual s name one study that set out to examine risk race recidivism predictive bias and disparate impact alleges a two fold 45 percent vs 23 percent adverse likelihood for black vs caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two year period of observation j lowenkamp c risk race recidivism predictive bias and disparate impact june 14 2016 available at ssrn https ssrn com abstract 2687339 or https doi org 10 2139 ssrn 2687339 in 2019 a research study revealed that a healthcare algorithm sold by optum favored white patients over sicker black patients algorithm predicts how much patients would cost health care system in future however cost is not race neutral as black patients incurred about 1 800 less in medical costs per year than white patients with same number of chronic conditions which led to algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases a study conducted by researchers at uc berkeley in november 2019 revealed that mortgage algorithms have been discriminatory towards latino and african americans which discriminated against minorities based on which is rooted in u s fair lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans these particular algorithms were present in fintech companies and were shown to discriminate against minorities online hate speech in 2017 a facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content according to internal facebook documents algorithm which is a combination of computer programs and human content reviewers was created to protect broad categories rather than specific subsets of categories for example posts denouncing muslims would be blocked while posts denouncing radical muslims would be allowed an unanticipated outcome of algorithm is to allow hate speech against black children because they denounce children subset of blacks rather than all blacks whereas all white men would trigger a block because whites and males are not considered subsets facebook was also found to allow ad purchasers to target jew haters as a category of users which company said was an inadvertent outcome of algorithms used in assessing and categorizing data company s design also allowed ad buyers to block african americans from seeing housing ads surveillance surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors and to determine who belongs in certain locations at certain times ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by racial diversity of images in its training database if majority of photos belong to one race or gender software is better at recognizing other members of that race or gender a 2002 analysis of software used to identify individuals in cctv images found several examples of bias when run against criminal databases software was assessed as identifying men more frequently than women older people more frequently than young and identified asians african americans and other races more often than whites additional studies of facial recognition software have found opposite to be true when trained on non criminal databases with software being least accurate in identifying darker skinned females sexual discrimination in 2011 users of gay hookup application grindr reported that android store s recommendation algorithm was linking grindr to applications designed to find sex offenders which critics said inaccurately related homosexuality with pedophilia writer mike criticized this association in atlantic arguing that such associations further stigmatized gay men in 2009 online retailer amazon de listed 57 000 books after an algorithmic change expanded its adult content blacklist to include any book addressing sexuality or gay themes such as critically acclaimed novel brokeback mountain in 2019 it was found that on facebook searches for photos of my female friends yielded suggestions such as in bikinis or at beach in contrast searches for photos of my male friends yielded no results facial recognition technology has been seen to cause problems for transgender individuals in 2018 there were reports of uber drivers who were transgender or transitioning experiencing difficulty with facial recognition software that uber implements as a built in security measure as a result of this some of accounts of trans uber drivers were suspended which cost them fares and potentially cost them a job all due to facial recognition software experiencing difficulties with recognizing face of a trans driver who was transitioning cite web url https www vox com future perfect 2019 4 19 ai bias facial recognition black gay transgender although solution to this issue would appear to be including trans individuals in training sets for machine learning models a instance of trans youtube videos that were collected to be used in training data did not receive consent from trans individuals that were included in videos which created an issue of violation of privacy there has also been a study that was conducted at stanford university in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images model in study predicted a correct distinction between gay and straight men 81 of time and a correct distinction between gay and straight women 74 of time this study resulted in a backlash from lgbtqia community who were fearful of possible negative repercussions that this ai system could have on individuals of lgbtqia community by putting individuals at risk of being outed against their will google search these results show that society as a whole hold a wide range of sexist ideas about women although users are ones generating these results to top of page google has failed to remove sexist and racist remarks in algorithms of oppression how search engines reinforce racism safiya noble notes an example when you google black girls a woman was searching internet for activities to entertain a preteen and her cousins of similar age after searching black girls images of pornography filled screen these results are a direct correlation to old media in a new media architecture due to google s algorithm it is unable to erase pages unless they qualify as unlawful obstacles to research several problems impede study of large scale algorithmic bias hindering application of academically rigorous studies and public understanding lack of transparency commercial algorithms are proprietary and may be treated as trade secrets treating algorithms as trade secrets protects companies such as search engines where a transparent algorithm might reveal tactics to manipulate search rankings this makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output complexity algorithmic processes are complex often exceeding understanding of people who use them large scale operations may not be understood even by those involved in creating them methods and processes of contemporary programs are often obscured by inability to know every permutation of a code s input or output social scientist bruno latour has identified this process as a process in which scientific and technical work is made invisible by its own success when a machine runs efficiently when a matter of fact is settled one need focus only on its inputs and outputs and not on its internal complexity thus paradoxically more science and technology succeed more opaque and obscure they become others have critiqued black box metaphor suggesting that current algorithms are not one black box but a network of interconnected ones an example of this complexity can be found in range of inputs into customizing feedback social media site facebook factored in at least 100 000 data points to determine layout of a user s social media feed in 2013 furthermore large teams of programmers may operate in relative isolation from one another and be unaware of cumulative effects of small decisions within connected elaborate algorithms not all code is original and may be borrowed from other libraries creating a complicated set of relationships between data processing and data input systems additional complexity occurs through machine learning and personalization of algorithms based on user interactions such as clicks time spent on site and other metrics these personal adjustments can confuse general attempts to understand algorithms one unidentified streaming radio service reported that it used five unique music selection algorithms it selected for its users based on their behavior this creates different experiences of same streaming services between different users making it harder to understand what these algorithms do companies also run frequent a b tests to fine tune algorithms based on user response for example search engine bing can run up to ten million subtle variations of its service per day creating different experiences of service between each use and or user lack of data about sensitive categories a significant barrier to understanding tackling of bias in practice is that categories such as demographics of individuals protected by anti discrimination law are often not explicitly considered when collecting and processing data in some cases there is little opportunity to collect this data explicitly such as in device fingerprinting ubiquitous computing and internet of things in other cases data controller may not wish to collect such data for reputational reasons or because it represents a heightened liability and security risk it may also be case that at least in relation to european union s general data protection regulation such data falls under special category provisions article 9 and therefore comes with more restrictions on potential collection and processing some practitioners have tried to estimate and these missing sensitive categorisations in order to allow bias mitigation for example building systems to infer ethnicity from names however this can introduce other forms of bias if not undertaken with care machine learning researchers have drawn upon cryptographic privacy enhancing technologies such as secure multi party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to in algorithmic bias does not only include protected categories but can also concerns characteristics less easily observable or such as political viewpoints in these cases there is rarely an easily accessible or non controversial ground truth and removing bias from such a system is more difficult furthermore false and accidental correlations can emerge from a lack of understanding of protected categories for example insurance rates based on historical data of car accidents which may overlap strictly by coincidence with residential clusters of ethnic minorities methods and tools there have been several attempts to create methods and tools that can detect and observe biases within an algorithm these emergent field focuses on tools which are typically applied to training data used by program rather than algorithm s internal processes these methods may also analyze a program s output and its usefulness and therefore may involve analysis of its confusion matrix or table of confusion https research google com attacking discrimination in ml attacking discrimination with smarter machine venturebeat com 2018 05 25 microsoft is developing a tool to help engineers catch bias in algorithms microsoft is developing a tool to help engineers catch bias in qz com facebook says it has a tool to detect bias in its artificial intelligence facebook says it has a tool to detect bias in its artificial source pymetrics audit venturebeat com cdn org c s venturebeat com 2018 05 31 pymetrics open sources audit ai an algorithm bias detection tool amp pymetrics open sources audit ai an algorithm bias detection github com aequitas open source aequitas bias and fairness audit uchicago edu aequitas open sources audit ai aequitas at university of www ibm com blogs research 2018 02 mitigating bias ai models mitigating bias in ai models currently a new ieee standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency i e to authorities or end users about function and possible effects of their algorithms project was approved february 2017 and is sponsored by software systems engineering standards committee a committee chartered by ieee computer society a draft of standard is expected to be submitted for balloting in june 2019 regulation europe general data protection regulation gdpr european union s revised data protection regime that was implemented in 2018 addresses automated individual decision making including profiling in article 22 these rules prohibit solely automated decisions which have a significant or legal effect on an individual unless they are explicitly authorised by consent contract or member state law where they are permitted there must be safeguards in place such as a right to a human in loop and a non binding right to an explanation of decisions reached while these regulations are commonly considered to be new nearly identical provisions have existed across europe since 1995 in article 15 of data protection directive original automated decision rules and safeguards found in french law since late 1970s gdpr addresses algorithmic bias in profiling systems as well as statistical approaches possible to clean it directly in recital 71 noting that controller should use appropriate mathematical or statistical procedures for profiling implement technical and organisational measures appropriate that prevents inter alia discriminatory effects on natural persons on basis of racial or ethnic origin political opinion religion or beliefs trade union membership genetic or health status or sexual orientation or that result in measures having such an effect like non binding right to an explanation in recital 71 problem is non binding nature of recitals while it has been treated as a requirement by article 29 working party that advised on implementation of data protection law its practical dimensions are unclear it has been argued that data protection impact assessments for high risk data profiling alongside other pre emptive measures within data protection may be a better way to tackle issues of algorithmic discrimination as it restricts actions of those deploying algorithms rather than requiring consumers to file complaints or request changes united states united states has no general legislation controlling algorithmic bias approaching problem through various state and federal laws that might vary by industry sector and by how an algorithm is used many policies are self enforced or controlled by federal trade commission in 2016 obama administration released national artificial intelligence research and development strategic plan which was intended to guide policymakers toward a critical assessment of algorithms it recommended researchers to design these systems so that their actions and decision making are transparent and easily interpretable by humans and thus can be examined for any bias they may contain rather than just learning and repeating these biases intended only as guidance report did not create any legal precedent in 2017 new york city passed first algorithmic accountability bill in united states bill which went into effect on january 1 2018 required creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with public and how agencies may address instances where people are harmed by agency automated decision systems task force is required to present findings and recommendations for further regulatory action in 2019 india on july 31 2018 a draft of personal data bill was presented draft proposes standards for storage processing and transmission of data while it does not use term algorithm it makes for provisions for harm resulting from any processing or any kind of processing undertaken by fiduciary it defines any denial or withdrawal of a service benefit or good resulting from an evaluative decision about data principal or any discriminatory treatment as a source of harm that could arise from improper use of data it also makes special provisions for people of intersex status https meity gov in files personal data protection bill 2018 pdf references further reading category machine learning category information ethics category computing and society category philosophy of artificial intelligence category discrimination category bias